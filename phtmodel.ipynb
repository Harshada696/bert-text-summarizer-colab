{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtS0cRrVwtT0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "azP8x9nFoNSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n"
      ],
      "metadata": {
        "id": "2riu2KkGwznc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "transformer = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS5Fo7w-wz_P",
        "outputId": "0d95fe31-99a3-4c6d-d42a-22f9d3d83487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionPooling(nn.Module):\n",
        "    def __init__(self, d_model=768, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "\n",
        "        self.query = nn.Parameter(torch.randn(n_heads, 1, self.d_head))\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Cp):\n",
        "        B, T, D = Cp.shape\n",
        "        Cp_heads = Cp.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        query = self.query.expand(self.n_heads, B, self.d_head).transpose(0, 1)\n",
        "        query = query.unsqueeze(2)\n",
        "\n",
        "        scores = torch.matmul(query, Cp_heads.transpose(-1, -2)) / (self.d_head ** 0.5)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        pooled = torch.matmul(attn_weights, Cp_heads)\n",
        "        pooled = pooled.squeeze(2).reshape(B, -1)\n",
        "\n",
        "        out = self.output_proj(pooled)\n",
        "        out = self.layernorm(out + self.ffn(out))\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "cSqT5abQw0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooling = MultiHeadAttentionPooling(d_model=768, n_heads=8)\n",
        "\n",
        "def encode_paragraphs(paragraph_texts):\n",
        "    inputs = tokenizer(paragraph_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = transformer(**inputs)\n",
        "    Cp = outputs.last_hidden_state\n",
        "    φp = pooling(Cp)\n",
        "    return Cp, φp\n"
      ],
      "metadata": {
        "id": "WK9RR3elw0Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paras = [\"This is paragraph one.\", \"This is paragraph two.\", \"This is paragraph three.\"]\n",
        "\n",
        "Cp_list, φp_list = [], []\n",
        "for p_text in paras:\n",
        "    Cp, φ = encode_paragraphs([p_text])\n",
        "    Cp_list.append(Cp)\n",
        "    φp_list.append(φ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHh2YC_zw-nI",
        "outputId": "6635ee3e-436a-4fdb-c833-358797737a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIxpXD7IxC_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PHTDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=768, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.word_cross_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.para_cross_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.norm4 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, memory_tokens, memory_paras):\n",
        "        self_attn_out, _ = self.self_attn(tgt, tgt, tgt)\n",
        "        tgt = self.norm1(tgt + self_attn_out)\n",
        "        word_attn_out, _ = self.word_cross_attn(tgt, memory_tokens, memory_tokens)\n",
        "        tgt = self.norm2(tgt + word_attn_out)\n",
        "        para_attn_out, _ = self.para_cross_attn(tgt, memory_paras, memory_paras)\n",
        "        tgt = self.norm3(tgt + para_attn_out)\n",
        "        ffn_out = self.ffn(tgt)\n",
        "        tgt = self.norm4(tgt + ffn_out)\n",
        "\n",
        "        return tgt\n"
      ],
      "metadata": {
        "id": "M16RiQscxDC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PHTDecoder(nn.Module):\n",
        "    def __init__(self, num_layers=3, d_model=768, n_heads=8, vocab_size=30522):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            PHTDecoderLayer(d_model, n_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_ids, memory_tokens, memory_paras):\n",
        "        tgt_emb = self.embedding(tgt_ids)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            tgt_emb = layer(tgt_emb, memory_tokens, memory_paras)\n",
        "\n",
        "        logits = self.out_proj(tgt_emb)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "qCImjK77xDSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cp_combined = torch.cat(Cp_list, dim=1)\n",
        "φp_combined = torch.stack(φp_list, dim=1)\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tgt_ids = tokenizer(\"summarize:\", return_tensors=\"pt\").input_ids\n",
        "decoder = PHTDecoder()\n",
        "logits = decoder(tgt_ids, Cp_combined, φp_combined)\n",
        "output_ids = torch.argmax(logits, dim=-1)\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnIR6gSHw-q_",
        "outputId": "ae65e30b-e8df-4e02-d8fb-4ca4a826079c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summary: jim shipment 犬rys 950 camille\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PHTEncoder(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased', d_model=768, n_heads=8):\n",
        "        super().__init__()\n",
        "        from transformers import BertModel, BertTokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.encoder = BertModel.from_pretrained(model_name)\n",
        "        self.pooling = MultiHeadAttentionPooling(d_model=d_model, n_heads=n_heads)\n",
        "\n",
        "    def forward(self, paras):\n",
        "        Cp_list, φp_list = [], []\n",
        "\n",
        "        for p in paras:\n",
        "            inputs = self.tokenizer(p, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "            outputs = self.encoder(**inputs)\n",
        "            Cp = outputs.last_hidden_state\n",
        "            φp = self.pooling(Cp)\n",
        "            Cp_list.append(Cp)\n",
        "            φp_list.append(φp)\n",
        "\n",
        "        Cp_combined = torch.cat(Cp_list, dim=1)\n",
        "        φp_combined = torch.stack(φp_list, dim=1)\n",
        "\n",
        "        return Cp_combined, φp_combined\n"
      ],
      "metadata": {
        "id": "DSAFupuuxJl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PHTModel(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, d_model=768, n_heads=8, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.encoder = PHTEncoder(d_model=d_model, n_heads=n_heads)\n",
        "        self.decoder = PHTDecoder(num_layers=num_layers, d_model=d_model, n_heads=n_heads, vocab_size=vocab_size)\n",
        "\n",
        "    def forward(self, paras, decoder_input_ids):\n",
        "        Cp, φp = self.encoder(paras)\n",
        "        logits = self.decoder(decoder_input_ids, Cp, φp)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "Hrd4a6m7xJpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paras = [\n",
        "    \"The Indian postal system plays a vital role in connecting rural and urban areas.\",\n",
        "    \"With digital adoption, tracking and speed have improved significantly.\",\n",
        "    \"However, many rural areas still lack proper infrastructure and daily services.\"\n",
        "]\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "decoder_input_ids = tokenizer(\"summarize:\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "model = PHTModel()\n",
        "logits = model(paras, decoder_input_ids)\n",
        "\n",
        "output_ids = torch.argmax(logits, dim=-1)\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7hgLR8axJsI",
        "outputId": "7f86ed9d-6ec8-4bcc-dcdb-5fbf9377e817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: submarines debate positions lowers comparison ref\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_21GPw7TxQoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z5qtvFzxQsE",
        "outputId": "2938d0a8-e0f2-4078-cdf8-0f1fc3a94ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bTKtLw3jxQvS",
        "outputId": "0c34654b-0480-4e4b-ec47-2566e426aa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-819e929e-8038-4046-a620-c73ec97b0291\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-819e929e-8038-4046-a620-c73ec97b0291\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"wb\") as f:\n",
        "    f.write(uploaded[\"kaggle.json\"])\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n"
      ],
      "metadata": {
        "id": "CEhGrDaExdUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"sandeep16064/wikisum\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIIil48bxdt1",
        "outputId": "b99f55ac-190b-47f5-c219-098c01a87e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/wikisum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/kaggle/input/wikisum\"\n",
        "print(\"Files in folder:\", os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7XmKd8ixdwx",
        "outputId": "646a637e-3a6f-4ee0-8c0c-4a0ddd49ed0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in folder: ['WikiSumDataset.jsonl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "json_path = '/kaggle/input/wikisum/WikiSumDataset.jsonl'\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "print(\"Keys:\", data[0].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoyQ2R73xd0A",
        "outputId": "824dc61d-5000-42ff-a61f-70ffdaf9c77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['url', 'title', 'summary', 'article', 'step_headers', 'fold'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = [item['article'] for item in data]\n",
        "summaries = [item['summary'] for item in data]\n",
        "titles = [item['title'] for item in data]\n"
      ],
      "metadata": {
        "id": "pq6mrG86xd3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_paragraphs(text):\n",
        "    return [p.strip() for p in text.split('\\n') if p.strip()]\n",
        "paragraphs_list = [split_into_paragraphs(article) for article in articles]\n"
      ],
      "metadata": {
        "id": "EsBny1GPxzcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First article title:\", titles[0])\n",
        "print(\"First 3 paragraphs:\", paragraphs_list[0][:3])\n",
        "print(\"Target summary:\", summaries[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpK5VoDRxzfP",
        "outputId": "7facb6ed-3b47-4ccc-c5e8-5368bba0c572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First article title: How to Store Fresh Oysters\n",
            "First 3 paragraphs: [\"Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40\\xa0°F (2 and 4\\xa0°C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5\\xa0in (1.3\\xa0cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad.\"]\n",
            "Target summary: To store fresh oysters, start by placing un-shucked oysters on top of a layer of ice in a bowl. Next, lay a damp towel on top of the bowl and place it in your refrigerator set to 35° to 40° Fahrenheit. Make sure that the towel stays damp and replace the ice as needed. Then, shuck and eat the oysters within 2 days. If your oysters are already shucked or you need to store them for more than 2 days, place them in the freezer until you're ready to use them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for paras, summary in zip(paragraphs_list, summaries):\n",
        "    dataset.append({\n",
        "        \"paragraphs\": paras,\n",
        "        \"summary\": summary\n",
        "    })\n"
      ],
      "metadata": {
        "id": "pO7aFcfKxzi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/wikisum_pht_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(dataset, f)\n",
        "\n",
        "print(\"Preprocessed dataset saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tcFCqa9xzmJ",
        "outputId": "d1cbf99f-7ad5-4f66-c322-54b1f60d49ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0]['paragraphs'][:2])\n",
        "print(\"Target:\", dataset[0]['summary'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woKktWkTx7uD",
        "outputId": "59b81fe6-9bc4-4893-dd63-c01cb3f201a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40\\xa0°F (2 and 4\\xa0°C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5\\xa0in (1.3\\xa0cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad.\"]\n",
            "Target: To store fresh oysters, start by placing un-shucked oysters on top of a layer of ice in a bowl. Next, lay a damp towel on top of the bowl and place it in your refrigerator set to 35° to 40° Fahrenheit. Make sure that the towel stays damp and replace the ice as needed. Then, shuck and eat the oysters within 2 days. If your oysters are already shucked or you need to store them for more than 2 days, place them in the freezer until you're ready to use them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \" \".join(dataset[0]['paragraphs'][:2])\n",
        "\n",
        "print(\"Input text length (chars):\", len(input_text))\n",
        "print(\"\\nInput for summarization:\\n\", input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jORJKFEQx7xG",
        "outputId": "20f4d35a-7406-48ff-807e-1674e5834ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text length (chars): 5740\n",
            "\n",
            "Input for summarization:\n",
            " Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40 °F (2 and 4 °C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5 in (1.3 cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PsXp_bxx702",
        "outputId": "c50dc882-1235-42ae-e08a-4098decbe0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZjy8nIyB6X",
        "outputId": "1704206e-a703-44df-c011-5a2e97a563c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t5_input = \"summarize: \" + input_text\n",
        "inputs = tokenizer_t5.encode(t5_input, return_tensors=\"pt\", max_length=512, truncation=True)\n"
      ],
      "metadata": {
        "id": "atRK_NCdyB97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_ids = model_t5.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "t5_summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"🔹 T5 Summary:\\n\", t5_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8rEMifNyCB5",
        "outputId": "0aed5adc-bc39-413f-8647-ec30f84a2f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 T5 Summary:\n",
            " if your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. leave the grit and dirt on the oysters. keep them moist and will help to insulate the meat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "inputs_bart = tokenizer_bart.encode(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "summary_ids_bart = model_bart.generate(inputs_bart, max_length=150, min_length=30, num_beams=4, early_stopping=True)\n",
        "\n",
        "bart_summary = tokenizer_bart.decode(summary_ids_bart[0], skip_special_tokens=True)\n",
        "print(\"BART Summary:\\n\", bart_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgntxLIyCFb",
        "outputId": "a02755d2-d933-477e-980f-e92735ad1825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BART Summary:\n",
            " Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. Keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If you don't have enough room in your freezer to keep full-shelled oysters, you can shucking them before storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "tokenizer_pegasus = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "model_pegasus = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "tokens_pegasus = tokenizer_pegasus(input_text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "summary_ids_pegasus = model_pegasus.generate(**tokens_pegasus)\n",
        "\n",
        "pegasus_summary = tokenizer_pegasus.decode(summary_ids_pegasus[0], skip_special_tokens=True)\n",
        "print(\"Pegasus Summary:\\n\", pegasus_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3dYWThhyCJC",
        "outputId": "e265dca4-4ce6-4fc6-8ca9-daec37ffd3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pegasus Summary:\n",
            " Here are some tips on how to safely store oysters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_distilbart = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model_distilbart = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "inputs_distilbart = tokenizer_distilbart.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids_distilbart = model_distilbart.generate(inputs_distilbart, max_length=150, min_length=30, num_beams=4, early_stopping=True)\n",
        "\n",
        "distilbart_summary = tokenizer_distilbart.decode(summary_ids_distilbart[0], skip_special_tokens=True)\n",
        "print(\" DistilBART Summary:\\n\", distilbart_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlGOSPXJyPJ_",
        "outputId": "27a92eb0-4647-4f2f-f0fd-5a9e7c475c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DistilBART Summary:\n",
            "  Oysters taste best when you shuck them immediately before eating them . Keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad . If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them . Dampen a towel with cold water and place it on top of the oysters with damp paper towels . If you don't have enough room in your freezer, you can shuck oysters before storage .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "tokenizer_mt5 = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model_mt5 = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "mt5_input = \"summarize: \" + input_text\n",
        "inputs_mt5 = tokenizer_mt5(mt5_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "\n",
        "summary_ids_mt5 = model_mt5.generate(inputs_mt5[\"input_ids\"], max_length=150, min_length=30, num_beams=4, early_stopping=True)\n",
        "mt5_summary = tokenizer_mt5.decode(summary_ids_mt5[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"mT5 Summary:\\n\", mt5_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YbMDjzHyPNo",
        "outputId": "7b1edf66-c47d-4070-9061-2d6a1e83c6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mT5 Summary:\n",
            " <extra_id_0> the oysters' juices easily. Continue Reading... Continue Reading... <extra_id_51> the oysters' juice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "flan_input = \"summarize: \" + dataset[0]['paragraphs'][0]\n",
        "flan_inputs = flan_tokenizer(flan_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "flan_summary_ids = flan_model.generate(flan_inputs['input_ids'], max_length=100, num_beams=4, early_stopping=True)\n",
        "flan_summary = flan_tokenizer.decode(flan_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"FLAN-T5 Summary:\\n\", flan_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbLlN-MHyPRu",
        "outputId": "885bc3a1-fdb8-4d9d-ec53-d7c12d209e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLAN-T5 Summary:\n",
            " Keep your oysters in their shells. Place ice in a bowl or container. Place your oysters on top of the ice bed. Place a towel on top of the oysters. Cover the oysters with a towel. Refrigerate your oysters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "longt5_tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-base\")\n",
        "longt5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/long-t5-tglobal-base\")\n",
        "\n",
        "longt5_input = \"summarize: \" + \" \".join(dataset[0]['paragraphs'])\n",
        "longt5_inputs = longt5_tokenizer(longt5_input, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
        "\n",
        "longt5_ids = longt5_model.generate(longt5_inputs['input_ids'], max_length=200, num_beams=4)\n",
        "longt5_summary = longt5_tokenizer.decode(longt5_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"LongT5 Summary:\\n\", longt5_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gafFxDyPVH",
        "outputId": "37ca63f9-6b3d-4960-f043-127a59c386b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongT5 Summary:\n",
            " You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
        "\n",
        "prophet_tokenizer = ProphetNetTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
        "prophet_model = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
        "\n",
        "prophet_input = prophet_tokenizer(\"summarize: \" + dataset[0]['paragraphs'][0], return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "prophet_summary_ids = prophet_model.generate(prophet_input['input_ids'], max_length=100, num_beams=4)\n",
        "prophet_summary = prophet_tokenizer.decode(prophet_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"ProphetNet Summary:\\n\", prophet_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beuGGt5eyYSk",
        "outputId": "c73a5f7d-0420-4855-f211-00e89f472cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n",
            "`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProphetNet Summary:\n",
            " keep your oysters in their shells. keep reading for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/wikisum_pht_dataset.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "WUQVsVcWyYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
        "\n",
        "led_tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
        "led_model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n",
        "\n",
        "led_input = \" \".join(dataset[0]['paragraphs'])\n",
        "led_inputs = led_tokenizer(led_input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=16384)\n",
        "\n",
        "led_summary_ids = led_model.generate(input_ids=led_inputs['input_ids'], attention_mask=led_inputs['attention_mask'], max_length=150, num_beams=4)\n",
        "led_summary = led_tokenizer.decode(led_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"LED Summary:\\n\", led_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS9RRkTGyYau",
        "outputId": "e51b91a5-a4d7-4a77-e66d-2dde49e6f312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n",
            "`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LED Summary:\n",
            " Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZHMs2w38wGO",
        "outputId": "4c6d0722-4c77-4bb4-9830-9333d37f3c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n"
      ],
      "metadata": {
        "id": "XoEtTCcN9NAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "uzjE00Kh-FAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers tokenizers\n",
        "!pip install transformers==4.38.2 tokenizers==0.18.1\n"
      ],
      "metadata": {
        "id": "JIhdJZ8H9NGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\")\n"
      ],
      "metadata": {
        "id": "H7AK6XSmyjFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0crMiE8kyjVG",
        "outputId": "ef27f1b9-46ff-41a9-ef90-9bd024dcd239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ]
    }
  ]
}